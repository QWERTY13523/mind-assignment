import os
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

# --- å¯¼å…¥ä¾èµ–åº“ ---
try:
    from py_sod_metrics import MAE, Emeasure, Fmeasure, Smeasure, WeightedFmeasure
except ImportError:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° py_sod_metrics åº“ã€‚è¯·è¿è¡Œ pip install py_sod_metrics")
    exit()

try:
    # å¯¼å…¥ä½ çš„ Config, Dataset å’Œ ä¸¤ä¸ªæ¨¡å‹å®šä¹‰
    from main import Config, CODDataset, BioCSTransNet
    from train_ablation import BioCSTransNet_Baseline
    from ablation_res import BioCS_ResNet
except ImportError:
    print("âŒ Error: æ— æ³•å¯¼å…¥æ¨¡å‹å®šä¹‰ï¼Œè¯·ç¡®ä¿ main.py and train_ablation.py åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")
    exit()

# ==========================================
# é…ç½®åŒºåŸŸ (æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹)
# ==========================================
# 1. æ•°æ®é›†æ ¹ç›®å½• (å‡è®¾ä½ çš„ç›®å½•ç»“æ„æ˜¯ ./cod/dataset/TestDataset/...)
TEST_ROOT = './dataset/TestDataset'

# 2. è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
MODELS_TO_EVAL = [
    {
        "name": "Baseline (Swin-B w/o CS)",
        "class": BioCSTransNet_Baseline,
        "path": "./checkpoints/best_model_baseline.pth"  # æ¶ˆèå®éªŒæƒé‡è·¯å¾„
    },
    {
        "name": "Bio-CSTransNet (Ours)",
        "class": BioCSTransNet,
        "path": "./checkpoints/best_model_3090.pth"  # å®Œæ•´æ¨¡å‹æƒé‡è·¯å¾„
    },
    {
        "name": "encoder-ResNet50",
        "class": BioCS_ResNet,
        "path": "./checkpoints_ablation_cnn/best_model_resnet_ablation.pth"  # å®Œæ•´æ¨¡å‹æƒé‡è·¯å¾„
    }
]

# 3. è¦æµ‹è¯•çš„æ•°æ®é›†åˆ—è¡¨
DATASETS_TO_EVAL = ['CAMO', 'COD10K']  # ä¹Ÿå¯ä»¥åŠ ä¸Š 'CHAMELEON'


# ==========================================
# æ ¸å¿ƒè¯„ä¼°å‡½æ•°
# ==========================================
def eval_one_model(model_class, weight_path, img_dir, mask_dir, device):
    # 1. åŠ è½½æ¨¡å‹
    model = model_class(num_classes=1).to(device)

    if not os.path.exists(weight_path):
        print(f"  âš ï¸ Warning: æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {weight_path}, è·³è¿‡æ­¤æ¨¡å‹...")
        return None

    checkpoint = torch.load(weight_path, map_location=device)
    state_dict = {k.replace("module.", ""): v for k, v in checkpoint.items()}
    model.load_state_dict(state_dict)
    model.eval()

    # 2. åŠ è½½æ•°æ®
    dataset = CODDataset(img_dir, mask_dir, is_train=False)
    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)

    # 3. åˆå§‹åŒ–æŒ‡æ ‡
    metrics = {
        "MAE": MAE(), "Sm": Smeasure(), "Fm": Fmeasure(), "Em": Emeasure(), "Wfm": WeightedFmeasure()
    }

    with torch.no_grad():
        # ä½¿ç”¨ tqdm ä½†ç¨å¾®ç®€åŒ–è¾“å‡ºä»¥å…åˆ·å±
        for images, masks in tqdm(loader, desc="    Inferring", leave=False):
            images = images.to(device)
            # å¤„ç† GT: [1, H, W] -> [H, W] numpy uint8
            gt_np = masks.cpu().numpy().squeeze()
            if gt_np.ndim == 3: gt_np = gt_np.squeeze()
            gt_np = (gt_np * 255).astype('uint8')

            # æ¨ç†
            outputs = model(images)
            pred = torch.sigmoid(outputs)

            # å¤„ç† Pred: [1, 1, H, W] -> [H, W] numpy float
            pred_np = pred.cpu().numpy().squeeze()
            if pred_np.ndim == 3: pred_np = pred_np.squeeze()

            # æ›´æ–°æ‰€æœ‰æŒ‡æ ‡
            metrics["MAE"].step(pred_np, gt_np)
            metrics["Sm"].step(pred_np, gt_np)
            metrics["Fm"].step(pred_np, gt_np)
            metrics["Em"].step(pred_np, gt_np)
            metrics["Wfm"].step(pred_np, gt_np)

    # 4. è·å–ç»“æœ
    results = {
        "Sm": metrics["Sm"].get_results()['sm'],
        "maxEm": metrics["Em"].get_results()['em']['curve'].max(),
        "WFm": metrics["Wfm"].get_results()['wfm'],
        "MAE": metrics["MAE"].get_results()['mae'],
        "maxFm": metrics["Fm"].get_results()['fm']['curve'].max()
    }
    return results


# ==========================================
# ä¸»æµç¨‹
# ==========================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Starting Comprehensive Evaluation on {device}...\n")

    # å­˜å‚¨æœ€ç»ˆçš„å¤§è¡¨æ ¼æ•°æ®
    final_report = {}

    for ds_name in DATASETS_TO_EVAL:
        print(f"ğŸ“‚ Processing Dataset: {ds_name} ...")

        # æ„é€ è·¯å¾„
        img_path = os.path.join(TEST_ROOT, ds_name, 'Imgs')
        gt_path = os.path.join(TEST_ROOT, ds_name, 'GT')

        if not os.path.exists(img_path):
            print(f"  âŒ Error: Dataset path not found: {img_path}")
            continue

        final_report[ds_name] = []

        for model_cfg in MODELS_TO_EVAL:
            print(f"  ğŸ¤– Evaluating Model: {model_cfg['name']} ...")

            res = eval_one_model(
                model_cfg['class'],
                model_cfg['path'],
                img_path,
                gt_path,
                device
            )

            if res:
                res['Model Name'] = model_cfg['name']
                final_report[ds_name].append(res)
                # æ‰“å°å•è¡Œç®€æŠ¥
                print(f"    -> [Result] Sm: {res['Sm']:.4f}, MAE: {res['MAE']:.4f}, maxF: {res['maxFm']:.4f}")
        print("-" * 50)

    # ==========================================
    # æ‰“å°æœ€ç»ˆ Markdown è¡¨æ ¼ (ç›´æ¥å¤åˆ¶åˆ°è®ºæ–‡/æŠ¥å‘Š)
    # ==========================================
    print("\n\n" + "#" * 20 + " FINAL REPORT " + "#" * 20)

    # è¡¨å¤´æ ¼å¼
    header = f"| {'Dataset':<10} | {'Model Architecture':<25} | {'S_alpha':<7} | {'maxE_phi':<8} | {'F_beta^w':<8} | {'MAE':<7} | {'maxF_beta':<9} |"
    separator = f"|{'-' * 12}|{'-' * 27}|{'-' * 9}|{'-' * 10}|{'-' * 10}|{'-' * 9}|{'-' * 11}|"

    print(header)
    print(separator)

    for ds_name, results in final_report.items():
        for res in results:
            row = f"| {ds_name:<10} | {res['Model Name']:<25} | {res['Sm']:.4f}  | {res['maxEm']:.4f}   | {res['WFm']:.4f}   | {res['MAE']:.4f}  | {res['maxFm']:.4f}    |"
            print(row)

    print("#" * 54)


if __name__ == '__main__':
    main()